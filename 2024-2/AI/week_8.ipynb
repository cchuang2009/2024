{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgnOPt_EE0Uz"
      },
      "outputs": [],
      "source": [
        "# chage the follwing link where your file was saved on your google drive\n",
        "\n",
        "share_=\"1LOviPZ8AcTR9ViquQuhpqcO65MHdLDmp\"\n",
        "url_head='https://drive.google.com/uc?id='\n",
        "\n",
        "dwn_url=url_head+share_"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gdown\n"
      ],
      "metadata": {
        "id": "fgJTUiqtFFgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown"
      ],
      "metadata": {
        "id": "yxPZGlGPFVgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_='traibing_0.csv.zip'\n",
        "data=file_\n",
        "gdown.download(dwn_url,data , quiet=False)"
      ],
      "metadata": {
        "id": "3tYNajWxFYGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "VThu9tnlFbfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/cchuang2009/tw_matplotlib.git\n",
        "import tw_matplotlib"
      ],
      "metadata": {
        "id": "XAFbMebdFnHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file= 'traibing_0.csv.zip'\n",
        "df= pd.read_csv(file, compression='zip')"
      ],
      "metadata": {
        "id": "5RPtQZXIFn-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target = 'é£†è‚¡'\n",
        "\n",
        "df_test=pd.read_csv(\"http://120.126.22.75/tbrain/public_x.csv.zip\")"
      ],
      "metadata": {
        "id": "c29HZf3iGenL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "pUkWa3YhGrrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# contenance df and df_test\n",
        "chunk=pd.concat([df, df_test], ignore_index=True)\n",
        "df_size=len(df)\n",
        "df_test_size=len(df_test)\n",
        "\n",
        "del df,df_test"
      ],
      "metadata": {
        "id": "xe870I8XG1t2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y= chunk[target]\n",
        "chunk= chunk.drop(columns=[target])\n"
      ],
      "metadata": {
        "id": "1weTe_VIG16X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# delete the object, ID , column\n",
        "drop_cols = ['ID']\n",
        "chunk.drop(drop_cols, axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "jfsMGP6kHLmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"df size:\", df_size)\n",
        "print(\"df_test size:\", df_test_size)\n",
        "print(\"chunk size:\", len(chunk))"
      ],
      "metadata": {
        "id": "JJb3LbghHLtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bin_feature_qcut(x, n_bins=100):\n",
        "    try:\n",
        "        return pd.qcut(x, q=n_bins, labels=False, duplicates=\"drop\")\n",
        "    except:\n",
        "        return pd.Series(np.nan, index=x.index)"
      ],
      "metadata": {
        "id": "3OKfPGZMHL54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the columns to file\n",
        "col_=list(chunk.columns)\n",
        "with open('columns.txt', 'w') as f:\n",
        "    for item in col_:\n",
        "        f.write(\"%s\\n\" % item)\n"
      ],
      "metadata": {
        "id": "suDCAnEvHlrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Load updated columns\n",
        "with open(\"columns.txt\", encoding=\"utf-8\") as f:\n",
        "    all_columns = [line.strip() for line in f.readlines()]\n",
        "\n",
        "# Categorize based on keyword hints\n",
        "categories = defaultdict(list)\n",
        "\n",
        "for col in all_columns:\n",
        "    if \"RSI\" in col:\n",
        "        categories[\"RSI\"].append(col)\n",
        "    elif \"MACD\" in col:\n",
        "        categories[\"MACD\"].append(col)\n",
        "    elif \"DIF\" in col:\n",
        "        categories[\"DIF\"].append(col)\n",
        "    elif \"ADX\" in col or \"DI\" in col:\n",
        "        categories[\"Trend Indicators\"].append(col)\n",
        "    elif \"ä¹–é›¢\" in col or \"åé›¢\" in col:\n",
        "        categories[\"BIAS\"].append(col)\n",
        "    elif \"ç‡Ÿæ”¶\" in col or \"è²¡å ±\" in col or \"EPS\" in col or \"ROE\" in col:\n",
        "        categories[\"Financial\"].append(col)\n",
        "    elif \"ä¸»åŠ›\" in col or \"è²·è³£è¶…\" in col:\n",
        "        categories[\"Chip/Main Force\"].append(col)\n",
        "    elif \"åˆ¸å•†\" in col or \"å¤–è³‡\" in col or \"æŠ•ä¿¡\" in col:\n",
        "        categories[\"Institution\"].append(col)\n",
        "    elif \"æˆäº¤é‡\" in col or \"æˆäº¤é‡‘é¡\" in col:\n",
        "        categories[\"Volume\"].append(col)\n",
        "    elif \"æŒ‡æ¨™\" in col:\n",
        "        categories[\"Technical_Other\"].append(col)\n",
        "    else:\n",
        "        categories[\"Other\"].append(col)"
      ],
      "metadata": {
        "id": "Gz8vcRERIDEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunk['å€‹è‚¡å‰1å¤©æˆäº¤é‡'].hist(bins=50)"
      ],
      "metadata": {
        "id": "J6C8_pXHITUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DFhgJSjVIYr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, roc_curve\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline"
      ],
      "metadata": {
        "id": "6ORmqbKPIYwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ“¦ Category-aware binning using qcut, log, or linear strategies\n",
        "def bin_features_by_category(df, categories, n_bins=100):\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "\n",
        "    df_binned = df.copy()\n",
        "\n",
        "    def safe_qcut(x):\n",
        "        try:\n",
        "            return pd.qcut(x, q=n_bins, labels=False, duplicates=\"drop\")\n",
        "        except:\n",
        "            return pd.Series(np.nan, index=x.index)\n",
        "\n",
        "    def safe_digitize(x):\n",
        "        x_clean = x.dropna()\n",
        "        if x_clean.empty:\n",
        "            return pd.Series(np.nan, index=x.index)\n",
        "        col_min, col_max = x_clean.min(), x_clean.max()\n",
        "        if col_min == col_max:\n",
        "            return pd.Series(0, index=x.index)\n",
        "        bins = np.linspace(col_min, col_max, n_bins + 1)\n",
        "        binned = np.digitize(x, bins[1:-1])\n",
        "        return pd.Series(binned, index=x.index)\n",
        "\n",
        "    def log_binning(x):\n",
        "        x_safe = x.copy()\n",
        "        x_safe = np.sign(x_safe) * np.log1p(np.abs(x_safe))\n",
        "        return safe_qcut(x_safe)\n",
        "\n",
        "    for cat, cols in categories.items():\n",
        "        for col in cols:\n",
        "            if col not in df.columns:\n",
        "                continue\n",
        "            if cat in [\"RSI\", \"MACD\", \"DIF\", \"BIAS\", \"Trend Indicators\", \"Chip/Main Force\", \"Institution\"]:\n",
        "                df_binned[col] = safe_qcut(df[col])\n",
        "            elif cat in [\"Volume\"]:\n",
        "                df_binned[col] = log_binning(df[col])\n",
        "            elif cat == \"Financial\":\n",
        "                df_binned[col] = log_binning(df[col])  # or safe_qcut(df[col]) if values are bounded\n",
        "            elif cat == \"Technical_Other\":\n",
        "                df_binned[col] = safe_qcut(df[col])\n",
        "            else:  # 'Other' or uncategorized\n",
        "                df_binned[col] = safe_digitize(df[col])\n",
        "\n",
        "    return df_binned"
      ],
      "metadata": {
        "id": "BzR-gwxsI3V_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 1: Prepare Dataset ---\n",
        "#df_size = 200864\n",
        "chunk = bin_features_by_category(chunk, categories, n_bins=100)  # bin all columns using qcut\n",
        "X_train = chunk.iloc[:df_size].copy()\n",
        "X_test = chunk.iloc[df_size:].copy()\n",
        "y_train = y[:df_size].copy()"
      ],
      "metadata": {
        "id": "K95BUUMAIgsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 2: Train CV RandomForest ---\n",
        "def train_cv_rf(X, y, X_test):\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    oof = np.zeros(len(X))\n",
        "    preds = np.zeros(len(X_test))\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "\n",
        "        #X_tr, y_tr = X.iloc[train_idx], y.iloc[train_idx]\n",
        "        #X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
        "\n",
        "        #sampler = ImbPipeline([\n",
        "        #    (\"over\", SMOTE(sampling_strategy=0.1, random_state=42)),\n",
        "        #    (\"under\", RandomUnderSampler(sampling_strategy=0.5, random_state=42))\n",
        "        #])\n",
        "        #X_res, y_res = sampler.fit_resample(X_tr, y_tr)\n",
        "\n",
        "        model = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n",
        "\n",
        "\n",
        "        model.fit(X.iloc[train_idx], y.iloc[train_idx])\n",
        "        #model = RandomForestClassifier(n_estimators=100, class_weight=None, random_state=42)\n",
        "        #model.fit(X_res, y_res)\n",
        "        prob = model.predict_proba(X.iloc[val_idx])[:, 1]\n",
        "        oof[val_idx] = prob\n",
        "        preds += model.predict_proba(X_test)[:, 1] / skf.n_splits\n",
        "        pred_label = (prob > 0.2).astype(int)\n",
        "        #prob = model.predict_proba(X_val)[:, 1]\n",
        "        #oof[val_idx] = prob\n",
        "        #preds += model.predict_proba(X_test)[:, 1] / skf.n_splits\n",
        "        #pred_label = (prob > 0.2).astype(int)\n",
        "        print(f\"[RF Fold {fold}] AUC: {roc_auc_score(y.iloc[val_idx], prob):.4f}, F1: {f1_score(y.iloc[val_idx], pred_label):.4f}, Acc: {accuracy_score(y.iloc[val_idx], pred_label):.4f}\")\n",
        "        #print(f\"[RF Fold {fold}] AUC: {roc_auc_score(y_val, prob):.4f}, F1: {f1_score(y_val, pred_label):.4f}, Acc: {accuracy_score(y_val, pred_label):.4f}\")\n",
        "\n",
        "    return oof, preds,model"
      ],
      "metadata": {
        "id": "lSxaIUftIqYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 5: Run All Models ---\n",
        "oof_rf, pred_rf, model_rf = train_cv_rf(X_train, y_train, X_test)"
      ],
      "metadata": {
        "id": "XKKgXuTDJJru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_rf[0]"
      ],
      "metadata": {
        "id": "75G7_1bxJvYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.tree import plot_tree\n",
        "\n",
        "plt.figure(figsize=(20, 10))\n",
        "plot_tree(model_rf.estimators_[0],\n",
        "          feature_names=X_train.columns.tolist(),\n",
        "          filled=True,\n",
        "          max_depth=3,\n",
        "          fontsize=8)\n",
        "plt.title(\"Example Tree from Random Forest\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pcO-Cob0JS7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming model_rf is trained and you have feature names\n",
        "importances = model_rf.feature_importances_\n",
        "features = X_train.columns\n",
        "\n",
        "# Sort and plot\n",
        "importance_df = pd.DataFrame({\"feature\": features, \"importance\": importances})\n",
        "importance_df = importance_df.sort_values(by=\"importance\", ascending=False).head(30)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.barh(importance_df[\"feature\"], importance_df[\"importance\"])\n",
        "plt.title(\"Top 30 Feature Importances (Random Forest)\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.xlabel(\"Importance\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yykVj0gCJz4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "thread=0.2\n",
        "df_test[target]=(preds_f1 > thread).astype(int)\n",
        "df_test.to_csv('submission-4-17.csv', index=False)"
      ],
      "metadata": {
        "id": "wcYro05VKmid"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}